# Simplified block cross-validation for time series in R

## Introduction
This repository introduces a simple cross-validation scheme for time series data in R which I developed as part of a term project in a class on Computational Statistics. The procedure extends the `createTimeSlices` function from the `caret` package (Kuhn et al., 2008), which allows to manually supply indices for the construction of the cross-validation samples.

Almost all machine learning methods rely on the analysis of an out-of-sample testing error to select the appropriate level of flexibility (“tuning”). For example, to choose the penalty parameter of LASSO or to select the depth of a tree, one usually assesses the prediction error on an independent test set. To avoid losing too much data from using a separate training and testing sample, the method most commonly used to perform such tests is cross-validation (CV). CV proceeds by randomly splitting the sample into a number of *k* subsets (folds) of approximately equal size. Then each of the subsets is used as a test set for a model that is trained on the remaining *k-1* folds. The overall prediction error is obtained by averaging over the errors in each of the *k* folds. In the most extreme case, *k* is set to the sample size such that every single observation is used once as a validation set. This is referred to as Leave-One-Out Cross-Validation (LOOCV). It can be shown that it is approximately unbiased for the true prediction error, since there is only a minimal loss from using *N -1* instead of *N* observations in the respective training sets.

However, the nice theoretical properties of cross-validation only hold in samples with independent and identically distributed observations. This is likely not satisfied when using time series data, even if the sample is stationary. Since stationarity requires that any two sub-series of equal length have the same joint distribution, it seems appropriate to assume that stationary time series are identically distributed (Bergmeir and Benítez, 2012). However, due to their time series nature, observations likely depend on each other over time even after taking first differences. Hence, we must resort to alternative validation methods for dependent data.
A broad range of different approaches to deal with cross-validation in time-series settings has been proposed in the literature. The simplest approach is to split the data into only one test set (usually a block at the end of the time horizon) and one training set (the remaining data). Then one can train different machine learners on the training data and compare their predictive performance on the test data. However, for model tuning of the individual learners, one needs further splits of the training data. To exploit the full information available in the training set, several time series cross-validation strategies have been proposed:

One approach is a forward rolling window as implemented by the `createTimeSlices` function in the `caret` package. This function defines an initial window of training observations at the beginning of the time period and a prediction horizon to test on. Then it iterates over the training observations, shifting the training window and testing period over time. While respecting the time dimension of the data, this approach has two main disadvantages. First, it does not account for the fact that observations in the training and test samples might not be independent. Second, it uses only training observations before the testing period which might be restrictive in small samples. 
One approach that has been argued to be asymptotically consistent with dependent data is the *hv*-block cross-validation proposed by Racine (2000). For a given observation *i*, this method takes an additional *v* observations from either side of *i* to form a validation set of size *n_v=1+2v*. Next, another *h* units from either side of the validation set are deleted such that *n_t = n-1-2v-2h* observations are used for the training set. This procedure is repeated for all observations *i*, leading to *n-2v* validation and training sets. The intuition for deleting *h* observations directly follows from the idea that autocorrelation in stationary time series depends only on the lag between the observations and decreases with increasing distance. Hence, it is assumed that by dropping a sufficient number of observations around the validation sample, independence between the training and validation samples can be established. The following figure illustrates five consecutive partitions of the approach for a sample of 120 time periods hand *h=v=* 12 (96 splits in total).

Since *hv*-block CV iterates over all observations in the sample, it is computationally expensive. Hence, I also implemented a less costly strategy that combines *k*-fold CV and *hv*-block CV as suggested by Bergmeir and Benítez (2012). In particular, they adjust the usual *k*-fold CV in two ways: Firstly, the observations are not reshuffled before splitting the data into *k* folds such that the time order is not distorted. Secondly, a number of *h* observations are deleted before and after the validation set as in the *hv*-block CV. The approach is illustrated in the following Figure for an exemplary sample of 120 months with *k*=5 and a gap of 12 time periods before and after the validation sample.
